essay.text <- fed.papers[indices]
for (j in 1:length(essay.text)) {
# ID intro to people of NY line
if(substr(essay.text[j], 1, nchar(new.york.text)) == new.york.text) {
has.new.york.text[i] <- 1
}
# ID Publius line
if(length(grep("PUBLIUS", essay.text[j], fixed=TRUE)) != 0) {
has.publius[i] <- 1
}
# ID Listed author
if(is.element(essay.text[j], possible.authors)) {
listed.authors[i] <- essay.text[j]
}
}
}
#
# Cut all text
# -Prior to "To the People of the State of New York:" line
# -After Publius signature
# We leverage the fact that all 85 essays started with this intro, and all but
# essay 37 ended with the Publius signature.  Note all footnotes are therefore
# removed.
#
# We then cut up each essay into paragraphs, and store each paragraphs
# separately.  Then store in new list 'fed.papers.list'.
#
fed.papers.list <- vector(n.essays, mode="list")
for (i in 1:n.essays) {
# Get text corresponding to that essay
indices <- essay.index[i, 1]:essay.index[i, 2]
essay.text <- fed.papers[indices]
# ID starting and ending indices of the essay
start.index <- 0
end.index <- 0
for(j in 1:length(essay.text)) {
if(substr(essay.text[j], 1, nchar(new.york.text)) == new.york.text) {
start.index <- j + 1
}
if(length(grep("PUBLIUS", essay.text[j], fixed=TRUE)) != 0) {
end.index <- j
}
}
# Essay 37 was not signed
if(i == 37) {
end.index <- length(essay.text)
}
# Drop Publius line
if (substr(essay.text[end.index], 1, nchar("PUBLIUS")) == "PUBLIUS" ) {
end.index <- end.index - 1
}
# Pare down text
essay.text <- essay.text[start.index:end.index]
# Remove all possible leading and trailing blank lines
while(essay.text[1] == "")
essay.text <- essay.text[-1]
while(essay.text[length(essay.text)] == "")
essay.text <- essay.text[-length(essay.text)]
# Determine number of paragraphs
n.paragraphs <- sum(essay.text == "") + 1
# Determine which lines start and end each paragraph
paragraph.starts <- c(1, which(essay.text == "") + 1)
paragraph.ends <- c(which(essay.text == "") - 1, length(essay.text))
paragraph.ends <- paragraph.starts - 1
paragraph.ends <- c(paragraph.ends, length(essay.text))
paragraph.ends <- paragraph.ends[-1]
# Store paragraphs in a list
paragraphs <- vector(n.paragraphs, mode="list")
for(j in 1:n.paragraphs) {
paragraphs[j] <-
paste(essay.text[paragraph.starts[j]:paragraph.ends[j]], collapse=" ")
}
fed.papers.list[[i]] <- paragraphs
}
#
# Store in nested tree/list structures the
# paragraph <- sentence <- word <- character counts
# Note:
# -sentences are cut using sentence token annotator function from package
# -all words dropped to lower case for counting purposes
# -words are assumed to be delineated by spaces
# -all punctuation dropped to count words.  This is an issue for Hamilton in
#  particular since he used words like "well-behaved" a few times
#
par.count <- sen.count <- word.count <- char.count <-
vector(length=n.essays, mode="list")
# Sentence tokenizer
sent_token_annotator <- Maxent_Sent_Token_Annotator()
for (i in 1:n.essays) {
essay <- fed.papers[[i]]
n.par <- length(essay)
par.count[[i]] <- n.par
# Further nested lists
sen.count[[i]] <- word.count[[i]] <- char.count[[i]] <-
vector(length=n.par, mode="list")
for (j in 1:n.par) {
par <- as.String(essay[[j]])
annotation <- annotate(par, sent_token_annotator)
sentences <- par[annotation]
n.sen <- length(sentences)
sen.count[[i]][[j]] <- n.sen
# Further nested lists
word.count[[i]][[j]] <- char.count[[i]][[j]] <-
vector(length=n.sen, mode="list")
for (k in 1:n.sen) {
sen <- sentences[[k]]
sen <- tolower(sen)
sen <- gsub("[[:punct:]]", " ", sen)
words <- unlist(strsplit(sen, " "))
words <- words[!words == ""]
words <- words[!words == " "]
n.words <- length(words)
word.count[[i]][[j]][[k]] <- n.words
# Further nested lists
char.count[[i]][[j]][[k]] <-
vector(length=n.words, mode="list")
for (l in 1:n.words){
char.count[[i]][[j]][[k]][[l]] <- nchar(words[l])
}
}
}
}
#
# Create data.frame of authors, disputed authors, and undisputed authors
#
hamilton.list <-
c("HAMILTON", "JAY", "JAY", "JAY", "JAY", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "MADISON", "HAMILTON", "HAMILTON", "HAMILTON",
"MADISON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON AND MADISON",
"HAMILTON AND MADISON", "HAMILTON AND MADISON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "MADISON", "MADISON", "MADISON", "MADISON",
"MADISON", "MADISON", "MADISON", "MADISON", "MADISON", "MADISON",
"MADISON", "MADISON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "JAY", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON")
madison.list <-
c("HAMILTON", "JAY", "JAY", "JAY", "JAY", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "MADISON", "HAMILTON", "HAMILTON", "HAMILTON",
"MADISON", "HAMILTON", "HAMILTON", "HAMILTON", "MADISON", "MADISON",
"MADISON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "MADISON",
"MADISON", "MADISON", "MADISON", "MADISON", "MADISON", "MADISON",
"MADISON", "MADISON", "MADISON", "MADISON", "MADISON", "MADISON",
"MADISON", "MADISON", "MADISON", "MADISON", "MADISON", "MADISON",
"MADISON", "MADISON", "MADISON", "HAMILTON", "HAMILTON", "HAMILTON",
"MADISON", "MADISON", "JAY", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON",
"HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON", "HAMILTON"
)
authors.array <- data.frame(essay=1:n.essays, hamilton=hamilton.list,
madison=madison.list, listed=listed.authors)
ls()
save(file="federalist.RData", authors.array, fed.papers, par.count, sen.count,
word.count, char.count)
rm(list=ls())
library(ggplot2)
load("federalist.RData")
#
# Focus for now on 69 out of 85 essays whose authorship is not disputed
#
disputed <-
which(as.character(authors.array$hamilton) != as.character(authors.array$madison))
undisputed <- setdiff(1:n.essays, disputed)
hamilton <- intersect(undisputed, which(authors.array$hamilton=="HAMILTON"))
madison <- intersect(undisputed, which(authors.array$hamilton=="MADISON"))
jay <- intersect(undisputed, which(authors.array$hamilton=="JAY"))
# re-order undisputed
undisputed <- c(hamilton, madison, jay)
authors <- c(rep("Hamilton", length(hamilton)),
rep("Madison", length(madison)),
rep("Jay", length(jay)))
initials <- c(rep("H", length(hamilton)),
rep("M", length(madison)),
rep("J"), length(jay))
hamilton.index <- which(authors=="Hamilton")
madison.index <- which(authors=="Madison")
jay.index <- which(authors=="Jay")
#
# Per essay metrics.  Nothing sexy except John Jay didn't do shit
#
n.par.per.essay <- unlist(par.count[undisputed])
boxplot(n.par.per.essay ~ authors, horizontal=TRUE,
xlab="# of paragraphs per essay")
n.sen.per.essay <- lapply(sen.count[undisputed], unlist)
n.sen.per.essay <- lapply(n.sen.per.essay, sum)
n.sen.per.essay <- unlist(n.sen.per.essay)
boxplot(n.sen.per.essay ~ authors, horizontal=TRUE,
xlab="# of sentences per essay")
n.word.per.essay <- lapply(word.count[undisputed], function(x){lapply(x, unlist)})
n.word.per.essay <- lapply(n.word.per.essay, unlist)
n.word.per.essay <- lapply(n.word.per.essay, sum)
n.word.per.essay <- unlist(n.word.per.essay)
boxplot(n.word.per.essay ~ authors, horizontal=TRUE, xlab="# of words per essay")
n.char.per.essay <- lapply(char.count[undisputed], function(x){lapply(x, unlist)})
n.char.per.essay <- lapply(n.char.per.essay, unlist)
n.char.per.essay <- lapply(n.char.per.essay, sum)
n.char.per.essay <- unlist(n.char.per.essay)
boxplot(n.char.per.essay ~ authors, horizontal=TRUE,
xlab="# of (non-space) characters per essay")
# # of total words and sentences
n.sen <- sum(n.sen.per.essay)
n.word <- sum(n.word.per.essay)
n.char <- sum(n.char.per.essay)
# Obviously highly correlated
n.per.essay <- data.frame(n.par=n.par.per.essay, n.sen=n.sen.per.essay,
n.word=n.word.per.essay)
round(cor(n.per.essay), 3)
# Lower correlations for non-Hamilton essays.  what is variance of correlations?
round(cor(n.per.essay[hamilton.index, ]), 3)
round(cor(n.per.essay[-hamilton.index, ]), 3)
n.essays <- length(fed.papers)
#
# Focus for now on 69 out of 85 essays whose authorship is not disputed
#
disputed <-
which(as.character(authors.array$hamilton) != as.character(authors.array$madison))
undisputed <- setdiff(1:n.essays, disputed)
hamilton <- intersect(undisputed, which(authors.array$hamilton=="HAMILTON"))
madison <- intersect(undisputed, which(authors.array$hamilton=="MADISON"))
jay <- intersect(undisputed, which(authors.array$hamilton=="JAY"))
# re-order undisputed
undisputed <- c(hamilton, madison, jay)
authors <- c(rep("Hamilton", length(hamilton)),
rep("Madison", length(madison)),
rep("Jay", length(jay)))
initials <- c(rep("H", length(hamilton)),
rep("M", length(madison)),
rep("J"), length(jay))
hamilton.index <- which(authors=="Hamilton")
madison.index <- which(authors=="Madison")
jay.index <- which(authors=="Jay")
#
# Per essay metrics.  Nothing sexy except John Jay didn't do shit
#
n.par.per.essay <- unlist(par.count[undisputed])
boxplot(n.par.per.essay ~ authors, horizontal=TRUE,
xlab="# of paragraphs per essay")
n.sen.per.essay <- lapply(sen.count[undisputed], unlist)
n.sen.per.essay <- lapply(n.sen.per.essay, sum)
n.sen.per.essay <- unlist(n.sen.per.essay)
boxplot(n.sen.per.essay ~ authors, horizontal=TRUE,
xlab="# of sentences per essay")
n.word.per.essay <- lapply(word.count[undisputed], function(x){lapply(x, unlist)})
n.word.per.essay <- lapply(n.word.per.essay, unlist)
n.word.per.essay <- lapply(n.word.per.essay, sum)
n.word.per.essay <- unlist(n.word.per.essay)
boxplot(n.word.per.essay ~ authors, horizontal=TRUE, xlab="# of words per essay")
n.char.per.essay <- lapply(char.count[undisputed], function(x){lapply(x, unlist)})
n.char.per.essay <- lapply(n.char.per.essay, unlist)
n.char.per.essay <- lapply(n.char.per.essay, sum)
n.char.per.essay <- unlist(n.char.per.essay)
boxplot(n.char.per.essay ~ authors, horizontal=TRUE,
xlab="# of (non-space) characters per essay")
# # of total words and sentences
n.sen <- sum(n.sen.per.essay)
n.word <- sum(n.word.per.essay)
n.char <- sum(n.char.per.essay)
# Obviously highly correlated
n.per.essay <- data.frame(n.par=n.par.per.essay, n.sen=n.sen.per.essay,
n.word=n.word.per.essay)
round(cor(n.per.essay), 3)
# Lower correlations for non-Hamilton essays.  what is variance of correlations?
round(cor(n.per.essay[hamilton.index, ]), 3)
round(cor(n.per.essay[-hamilton.index, ]), 3)
#
# Per sentence metrics:  average sentence length
#
# Words per sentence.  Hard to figure out how to keep the exchangeable units
# intact
n.word.per.sen <- unlist(word.count[undisputed])
n.word.per.sen <- unlist(n.word.per.sen)
# sanity check:  check number of sentences
stopifnot(n.sen == length(n.word.per.sen))
# For each sentence associate author
authors.sen <- rep(authors, times=n.sen.per.essay)
#
# Analysis on Federalist Papers
#
library(ggplot2)
load("federalist.RData")
n.essays <- length(fed.papers)
#
# Focus for now on 69 out of 85 essays whose authorship is not disputed
#
disputed <-
which(as.character(authors.array$hamilton) != as.character(authors.array$madison))
undisputed <- setdiff(1:n.essays, disputed)
hamilton <- intersect(undisputed, which(authors.array$hamilton=="HAMILTON"))
madison <- intersect(undisputed, which(authors.array$hamilton=="MADISON"))
jay <- intersect(undisputed, which(authors.array$hamilton=="JAY"))
# re-order undisputed
undisputed <- c(hamilton, madison, jay)
authors <- c(rep("Hamilton", length(hamilton)),
rep("Madison", length(madison)),
rep("Jay", length(jay)))
initials <- c(rep("H", length(hamilton)),
rep("M", length(madison)),
rep("J"), length(jay))
hamilton.index <- which(authors=="Hamilton")
madison.index <- which(authors=="Madison")
jay.index <- which(authors=="Jay")
#
# Per essay metrics.  Nothing sexy except John Jay didn't do shit
#
n.par.per.essay <- unlist(par.count[undisputed])
boxplot(n.par.per.essay ~ authors, horizontal=TRUE,
xlab="# of paragraphs per essay")
n.sen.per.essay <- lapply(sen.count[undisputed], unlist)
n.sen.per.essay <- lapply(n.sen.per.essay, sum)
n.sen.per.essay <- unlist(n.sen.per.essay)
boxplot(n.sen.per.essay ~ authors, horizontal=TRUE,
xlab="# of sentences per essay")
n.word.per.essay <- lapply(word.count[undisputed], function(x){lapply(x, unlist)})
n.word.per.essay <- lapply(n.word.per.essay, unlist)
n.word.per.essay <- lapply(n.word.per.essay, sum)
n.word.per.essay <- unlist(n.word.per.essay)
boxplot(n.word.per.essay ~ authors, horizontal=TRUE, xlab="# of words per essay")
n.char.per.essay <- lapply(char.count[undisputed], function(x){lapply(x, unlist)})
n.char.per.essay <- lapply(n.char.per.essay, unlist)
n.char.per.essay <- lapply(n.char.per.essay, sum)
n.char.per.essay <- unlist(n.char.per.essay)
boxplot(n.char.per.essay ~ authors, horizontal=TRUE,
xlab="# of (non-space) characters per essay")
undisputed
n.par.per.essay
par.count
rm(list=ls())
#
# This R script preprocesses the following document:
# http://thomas.loc.gov/home/histdox/fedpaper.txt
# containing the Federalist Papers for analysis in R
#
# Page 5 of
# http://writing.mit.edu/sites/writing.mit.edu/files/Who%20Wrote%20the%20Federalist%20Papers.pdf
# contains the two conflicting list of authors of the papers
#
# Libraries need for text processing
library(openNLP) ## Loads the package for use in the task
library(openNLPmodels.en) ## Loads the model files for the English language
library(NLP)
# Input document: multiple line document
fed.papers <- scan("pg1404.txt", "character", sep="\n", blank.lines.skip = FALSE)
#
# First pass to identify starting and ending line of each essay
#
essay.starts <- NULL
for (i in 1:length(fed.papers)) {
if (substr(fed.papers[i], 1, nchar("FEDERALIST No.")) == "FEDERALIST No.") {
essay.starts <- c(essay.starts, i)
}
}
essay.ends <- essay.starts - 1
essay.ends <- essay.ends[-1]
essay.ends <- c(essay.ends, length(fed.papers))
essay.index <- cbind(essay.starts, essay.ends)
# 85 essays total
n.essays <- nrow(essay.index)
#
# Identify
# -Which line has "To the People of the State of New York" intro (if at all)
# -Which line has Publius signature
# -Listed author
#
new.york.text <- "To the People of the State of New York"
has.new.york.text <- rep(0, n.essays)
has.publius <- rep(0, n.essays)
listed.authors <- rep("", n.essays)
possible.authors <- c("HAMILTON", "JAY", "MADISON", "HAMILTON OR MADISON",
"MADISON, with HAMILTON")
for (i in 1:n.essays) {
indices <- essay.index[i, 1] : essay.index[i, 2]
essay.text <- fed.papers[indices]
for (j in 1:length(essay.text)) {
# ID intro to people of NY line
if(substr(essay.text[j], 1, nchar(new.york.text)) == new.york.text) {
has.new.york.text[i] <- 1
}
# ID Publius line
if(length(grep("PUBLIUS", essay.text[j], fixed=TRUE)) != 0) {
has.publius[i] <- 1
}
# ID Listed author
if(is.element(essay.text[j], possible.authors)) {
listed.authors[i] <- essay.text[j]
}
}
}
#
# Cut all text
# -Prior to "To the People of the State of New York:" line
# -After Publius signature
# We leverage the fact that all 85 essays started with this intro, and all but
# essay 37 ended with the Publius signature.  Note all footnotes are therefore
# removed.
#
# We then cut up each essay into paragraphs, and store each paragraphs
# separately.  Then store in new list 'fed.papers.list'.
#
fed.papers.list <- vector(n.essays, mode="list")
for (i in 1:n.essays) {
# Get text corresponding to that essay
indices <- essay.index[i, 1]:essay.index[i, 2]
essay.text <- fed.papers[indices]
# ID starting and ending indices of the essay
start.index <- 0
end.index <- 0
for(j in 1:length(essay.text)) {
if(substr(essay.text[j], 1, nchar(new.york.text)) == new.york.text) {
start.index <- j + 1
}
if(length(grep("PUBLIUS", essay.text[j], fixed=TRUE)) != 0) {
end.index <- j
}
}
# Essay 37 was not signed
if(i == 37) {
end.index <- length(essay.text)
}
# Drop Publius line
if (substr(essay.text[end.index], 1, nchar("PUBLIUS")) == "PUBLIUS" ) {
end.index <- end.index - 1
}
# Pare down text
essay.text <- essay.text[start.index:end.index]
# Remove all possible leading and trailing blank lines
while(essay.text[1] == "")
essay.text <- essay.text[-1]
while(essay.text[length(essay.text)] == "")
essay.text <- essay.text[-length(essay.text)]
# Determine number of paragraphs
n.paragraphs <- sum(essay.text == "") + 1
# Determine which lines start and end each paragraph
paragraph.starts <- c(1, which(essay.text == "") + 1)
paragraph.ends <- c(which(essay.text == "") - 1, length(essay.text))
paragraph.ends <- paragraph.starts - 1
paragraph.ends <- c(paragraph.ends, length(essay.text))
paragraph.ends <- paragraph.ends[-1]
# Store paragraphs in a list
paragraphs <- vector(n.paragraphs, mode="list")
for(j in 1:n.paragraphs) {
paragraphs[j] <-
paste(essay.text[paragraph.starts[j]:paragraph.ends[j]], collapse=" ")
}
fed.papers.list[[i]] <- paragraphs
}
#
# Store in nested tree/list structures the
# paragraph <- sentence <- word <- character counts
# Note:
# -sentences are cut using sentence token annotator function from package
# -all words dropped to lower case for counting purposes
# -words are assumed to be delineated by spaces
# -all punctuation dropped to count words.  This is an issue for Hamilton in
#  particular since he used words like "well-behaved" a few times
#
par.count <- sen.count <- word.count <- char.count <-
vector(length=n.essays, mode="list")
# Sentence tokenizer
sent_token_annotator <- Maxent_Sent_Token_Annotator()
for (i in 1:n.essays) {
essay <- fed.papers[[i]]
n.par <- length(essay)
par.count[[i]] <- n.par
# Further nested lists
sen.count[[i]] <- word.count[[i]] <- char.count[[i]] <-
vector(length=n.par, mode="list")
for (j in 1:n.par) {
par <- as.String(essay[[j]])
annotation <- annotate(par, sent_token_annotator)
sentences <- par[annotation]
n.sen <- length(sentences)
sen.count[[i]][[j]] <- n.sen
# Further nested lists
word.count[[i]][[j]] <- char.count[[i]][[j]] <-
vector(length=n.sen, mode="list")
for (k in 1:n.sen) {
sen <- sentences[[k]]
sen <- tolower(sen)
sen <- gsub("[[:punct:]]", " ", sen)
words <- unlist(strsplit(sen, " "))
words <- words[!words == ""]
words <- words[!words == " "]
n.words <- length(words)
word.count[[i]][[j]][[k]] <- n.words
# Further nested lists
char.count[[i]][[j]][[k]] <-
vector(length=n.words, mode="list")
for (l in 1:n.words){
char.count[[i]][[j]][[k]][[l]] <- nchar(words[l])
}
}
}
}
essay <- fed.papers[[i]]
n.par <- length(essay)
par.count[[i]] <- n.par
# Further nested lists
sen.count[[i]] <- word.count[[i]] <- char.count[[i]] <-
vector(length=n.par, mode="list")
par <- as.String(essay[[j]])
annotation <- annotate(par, sent_token_annotator)
sentences <- par[annotation]
n.sen <- length(sentences)
sen.count[[i]][[j]] <- n.sen
